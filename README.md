ask_ollama(question) → one-shot question → one-shot answer.

chat_with_ollama() → ongoing conversation with memory (chatbot mode).

Ollama runs the model locally, so there’s no external API key needed.

The model must be pulled first (e.g., ollama pull llama3.2).
